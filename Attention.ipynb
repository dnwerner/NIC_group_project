{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0",
      "metadata": {
        "id": "0"
      },
      "source": [
        "# Attention mechanisms"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1",
      "metadata": {
        "id": "1"
      },
      "source": [
        "## Setup for Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2",
      "metadata": {
        "id": "2"
      },
      "source": [
        "If you are running in Google Colab, you need to run the following setup script. It will set up the environment and then kill the kernel to force a kernel restart. Do not panic when this happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3",
      "metadata": {
        "id": "3"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    !rm -rf tutorial_repo\n",
        "    !git clone --recursive https://github.com/Benkendorfer/ErUM_Transformers_Tutorial_2025.git tutorial_repo\n",
        "    !mv tutorial_repo/data data\n",
        "    !mv tutorial_repo/utilities utilities\n",
        "\n",
        "    import subprocess\n",
        "    import sys\n",
        "\n",
        "    process = subprocess.Popen(\n",
        "        [sys.executable, \"-m\", \"pip\", \"install\", \"torch==2.2.0\", \"torchtext==0.17.0\", \"numpy<2\"],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True\n",
        "    )\n",
        "\n",
        "    restart_needed = False\n",
        "    for line in process.stdout:\n",
        "        print(line, end=\"\")  # stream output to notebook\n",
        "        if any(phrase in line for phrase in [\n",
        "            \"Installing collected packages\",\n",
        "            \"Successfully installed\",\n",
        "            \"upgraded\",\n",
        "            \"added\"\n",
        "        ]):\n",
        "            restart_needed = True\n",
        "\n",
        "    process.wait()\n",
        "\n",
        "    import os\n",
        "\n",
        "    if restart_needed:\n",
        "        print(\"Restarting Colab runtime to finalize package installation...\")\n",
        "        os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4",
      "metadata": {
        "id": "4"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5",
      "metadata": {
        "id": "5"
      },
      "source": [
        "We will use a dataset from <https://github.com/Charlie9/enron_intent_dataset_verified?tab=readme-ov-file>. This dataset consists of sentences from emails sent between employees of the Enron corporation. Each sentence has been manually labeled regarding whether it contains a request or does not contain a request. We will train an attention model to classify sentences as \"request\" or \"no request\" sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6",
      "metadata": {
        "id": "6"
      },
      "outputs": [],
      "source": [
        "def read_intent_file(file_path: str) -> list[str]:\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    return [line.strip() for line in lines]\n",
        "\n",
        "# Read positive and negative intent files\n",
        "pos_intent_path = \"data/Enron/intent_pos\"\n",
        "neg_intent_path = \"data/Enron/intent_neg\"\n",
        "\n",
        "pos_intent_sentences = read_intent_file(pos_intent_path)\n",
        "neg_intent_sentences = read_intent_file(neg_intent_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7",
      "metadata": {
        "id": "7"
      },
      "source": [
        "Take a look at some of these sentences. Does the dataset look as you would expect?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8",
      "metadata": {
        "id": "8"
      },
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "    # print out some sentences and remove the ...\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9",
      "metadata": {
        "id": "9"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10",
      "metadata": {
        "id": "10"
      },
      "source": [
        "Now that we have the sentences, we need to parse them into tokens that can be fed to the model. Tokenization is a surprisingly complicated task which is highly language-dependent.\n",
        "\n",
        "(It is not as simple as identifying words; often parts of words are themselves individual tokens. The past-tense marker `-ed` must be separated from the verb `trained`, for example, to create two tokens: `train` and `ed`. German, then, requires a different algorithm --- the word `trainiert` clearly has the token `t` at the end, but then should the tokens be `trainieren` and `t`? Or `trainier` and `t`?).\n",
        "\n",
        "Luckily, we are physicists rather than linguists, and some very clever people have done the work already."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11",
      "metadata": {
        "id": "11"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "tokens = tokenizer(\"Please send me the report by EOD.\")\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12",
      "metadata": {
        "id": "12"
      },
      "source": [
        "Tokenize some sentences of your choosing and see what happens. Does it work as you expect, or are there any surprises? What if you try to tokenize a non-English sentence?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13",
      "metadata": {
        "id": "13"
      },
      "outputs": [],
      "source": [
        "# Tokenize some sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14",
      "metadata": {
        "id": "14"
      },
      "source": [
        "Now that we can tokenize individual sentences, we need to build up a vocabulary of tokens that appear in our training data. We will also add two new tokens to the vocabulary --- a token `<unk>` representing an unknown input, and a token `<pad>` for whitespace padding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15",
      "metadata": {
        "id": "15"
      },
      "outputs": [],
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for txt in data_iter:\n",
        "        yield tokenizer(txt)\n",
        "\n",
        "all_sentences = pos_intent_sentences + neg_intent_sentences\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(all_sentences), specials=[\"<unk>\", \"<pad>\"])\n",
        "\n",
        "# We set the default token to be <unk>\n",
        "vocab.set_default_index(vocab['<unk>'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16",
      "metadata": {
        "id": "16"
      },
      "source": [
        "Take a look at a few entries of the `vocab` object. What sort of object is it? What does it map words onto?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17",
      "metadata": {
        "id": "17"
      },
      "outputs": [],
      "source": [
        "# Look at some vocab entries\n",
        "\n",
        "# print(vocab['the'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18",
      "metadata": {
        "id": "18"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19",
      "metadata": {
        "id": "19"
      },
      "source": [
        "Now let's train an attention-based model to classify sentences as requests or not. We will train an extremely simple model with a single attention head, just to show you how it all works.\n",
        "\n",
        "First, we need to load the data into a PyTorch `DataLoader` that can be passed to the model. This is necessary for easy parallelization of the training (though it is not critical for us in this application).\n",
        "\n",
        "We have hidden some technical details in the `data_management.py` file. Feel free to look at the implementation in there if you are curious."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20",
      "metadata": {
        "id": "20"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from utilities.data_management import EnronRequestDataset, collate_fn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Gather sentences and assign labels\n",
        "sentences = pos_intent_sentences + neg_intent_sentences\n",
        "labels = [1] * len(pos_intent_sentences) + [0] * len(neg_intent_sentences)\n",
        "\n",
        "# Wrap the sentences in a DataLoader object\n",
        "dataset = EnronRequestDataset(sentences, labels, vocab, tokenizer)\n",
        "loader  = DataLoader(dataset,\n",
        "                     batch_size=32,\n",
        "                     shuffle=True,\n",
        "                     collate_fn=lambda batch: collate_fn(batch, vocab),\n",
        "                     num_workers=0,\n",
        "                     pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21",
      "metadata": {
        "id": "21"
      },
      "source": [
        "Now we can actually train the model! In order to best understand what is happening, we will use a very simple attention mechanism attached to a multilayer perceptron. The implementation has been hidden in the file `attention_model.py` --- feel free to look in there if you'd like! It's not as scary as you might think.\n",
        "\n",
        "For now, we will focus on trying to understand what the model is doing. First, we will load the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22",
      "metadata": {
        "id": "22"
      },
      "outputs": [],
      "source": [
        "import utilities.attention_model as attention_model\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "model = attention_model.RequestClassifier(len(vocab), num_layers=1, d_attention=64, d_embedding=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23",
      "metadata": {
        "id": "23"
      },
      "source": [
        "What does the model look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24",
      "metadata": {
        "id": "24"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25",
      "metadata": {
        "id": "25"
      },
      "source": [
        "Now let's see how the untrained model performs on the dataset. We can define a simple function to compute the true and false positive rates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26",
      "metadata": {
        "id": "26"
      },
      "outputs": [],
      "source": [
        "# Compute true positives, false positives, true negatives, and false negatives\n",
        "def compute_metrics(preds, labels):\n",
        "    tp = ((preds == 1) & (labels == 1)).sum().item()\n",
        "    fp = ((preds == 1) & (labels == 0)).sum().item()\n",
        "    tn = ((preds == 0) & (labels == 0)).sum().item()\n",
        "    fn = ((preds == 0) & (labels == 1)).sum().item()\n",
        "    return tp, fp, tn, fn\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, labels, pad_mask in data_loader:\n",
        "            logits = model(src, src_key_padding_mask=pad_mask)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            all_preds.append(preds)\n",
        "            all_labels.append(labels)\n",
        "\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    tp, fp, tn, fn = compute_metrics(all_preds, all_labels)\n",
        "    print(f\"True positive rate: {tp / (tp + fn) * 100:.2f}%\")\n",
        "    print(f\"False positive rate: {fp / (fp + tn) * 100:.2f}%\")\n",
        "    print(\"\")\n",
        "    print(f\"True negative rate: {tn / (tn + fp) * 100:.2f}%\")\n",
        "    print(f\"False negative rate: {fn / (fn + tp) * 100:.2f}%\")\n",
        "\n",
        "evaluate_model(model, loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27",
      "metadata": {
        "id": "27"
      },
      "source": [
        "You should have found that the true and false positive (and true and false negative) are quite bad. This is expected for the untrained model --- it is just randomly guessing.\n",
        "\n",
        "Now let's perform the training. We can train the model over a small number of epochs using a simple binary cross-entropy loss function. Play around with the learning rate and number of epochs --- what do you find in terms of performance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28",
      "metadata": {
        "id": "28"
      },
      "outputs": [],
      "source": [
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    loss = torch.tensor(0.0)\n",
        "    for src, labels, pad_mask in loader:\n",
        "        logits = model(src, src_key_padding_mask=pad_mask)\n",
        "        loss   = loss_fn(logits, labels)\n",
        "\n",
        "        if torch.isnan(logits).any():\n",
        "            print(\"ðŸ›‘ NaN in logits!\"); break\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step(); opt.zero_grad()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29",
      "metadata": {
        "id": "29"
      },
      "source": [
        "We have trained a very simple model as a proof-of-concept, but think about what might be missing here, using what you have learned from the other tutorials in the workshop. Would you want to deploy this model in the real world? How could we make it better?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30",
      "metadata": {
        "id": "30"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31",
      "metadata": {
        "id": "31"
      },
      "source": [
        "Now that the model is trained, we can evaluate its performance. First, we should compute the true positive and false positive rates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32",
      "metadata": {
        "id": "32"
      },
      "outputs": [],
      "source": [
        "evaluate_model(model, loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33",
      "metadata": {
        "id": "33"
      },
      "source": [
        "You probably found very good performance (unless something went wrong with the training). In fact, the true positive and true negative rates might be 100%. This is indicative of some possible overfitting in the model --- how could we avoid overfitting for future training?\n",
        "\n",
        "Now that we have the trained model, we can play around with it a bit. Let's write a function to evaluate a single sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34",
      "metadata": {
        "id": "34"
      },
      "outputs": [],
      "source": [
        "# Evaluate single sentence\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def predict_sentence(model, sentence, vocab, tokenizer) -> tuple[int, list[float]]:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Tokenize and map to vocabulary\n",
        "        tokens = tokenizer(sentence)\n",
        "        ids    = torch.tensor(vocab(tokens), dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "        # Build padding mask (necessary to pad empty characters)\n",
        "        pad_idx = vocab['<pad>']\n",
        "        mask    = ids != pad_idx\n",
        "\n",
        "        # Run the transformer\n",
        "        logits = model(ids, src_key_padding_mask=~mask)\n",
        "        probs  = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Return the class with the highest probability (0 or 1)\n",
        "        pred   = probs.argmax(dim=-1).item()\n",
        "\n",
        "    # Convert prediction to int\n",
        "    pred = int(pred)\n",
        "\n",
        "    return pred, probs.squeeze().tolist()\n",
        "\n",
        "def print_prediction_for_sentence(sentence, whitespace=True):\n",
        "    pred, probs = predict_sentence(model, sentence, vocab, tokenizer)\n",
        "\n",
        "    label_map = {0: \"no_request\", 1: \"request\"}\n",
        "    print(f\"â†’ {sentence!r}\")\n",
        "    print(f\"Prediction: {label_map[pred]} (P(request)={probs[1]:.4f})\")\n",
        "\n",
        "    if whitespace:\n",
        "        print()\n",
        "\n",
        "test_sentences = [\n",
        "    \"Please send me the report today by the end of the day.\",\n",
        "    \"I need the report as soon as possible.\",\n",
        "    \"Can you send me the report?\",\n",
        "    \"The weather is nice today.\",\n",
        "    \"Knut is teaching a lecture on transformers.\",\n",
        "    \"Student, please evaluate the model performance.\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    print_prediction_for_sentence(sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35",
      "metadata": {
        "id": "35"
      },
      "source": [
        "This is looking pretty good (but not perfect)! Try playing around now with your own sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36",
      "metadata": {
        "id": "36"
      },
      "outputs": [],
      "source": [
        "# Write a request that you might find in a business email (so it uses words that might be in the vocab)\n",
        "sentence = \"Write a request here.\"\n",
        "print_prediction_for_sentence(sentence)\n",
        "\n",
        "# Write a non-request that you might find in a business email\n",
        "sentence = \"Write a non-request here.\"\n",
        "print_prediction_for_sentence(sentence)\n",
        "\n",
        "# Write a request with some words that you think might not be in the vocab\n",
        "sentence = \"Write a request with non-vocab words here.\"\n",
        "print_prediction_for_sentence(sentence)\n",
        "\n",
        "# Write a request in another language (e.g., German)\n",
        "sentence = \"Schreiben Sie eine Anfrage in einer anderen Sprache hier.\"\n",
        "print_prediction_for_sentence(sentence)\n",
        "\n",
        "# Write any sentence of your choosing\n",
        "sentence = \"Your sentence here.\"\n",
        "print_prediction_for_sentence(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37",
      "metadata": {
        "id": "37"
      },
      "source": [
        "What do you notice about the model behavior? Do certain words seem to cause the model to predict a sentence as being more request-like? Do certain requests consistently fail? Can you figure out how to modify an incorrectly-classified request in order to make it classify correctly?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38",
      "metadata": {
        "id": "38"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39",
      "metadata": {
        "id": "39"
      },
      "source": [
        "Let's try to visualize what the attention mechanism is doing. For the following sections, if you have changed the topology of your classifier model, make sure that there is only one attention layer.\n",
        "\n",
        "Let's use an example sentence that evaluates as a prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40",
      "metadata": {
        "id": "40"
      },
      "outputs": [],
      "source": [
        "visualization_sentence = \"Please send me this email today.\"\n",
        "print_prediction_for_sentence(visualization_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41",
      "metadata": {
        "id": "41"
      },
      "source": [
        "The attention model has been written in such a way that we can extract the attention matrix from the last attention layer of the network. Let's grab it now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42",
      "metadata": {
        "id": "42"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_attention_matrix(model, sentence, vocab, tokenizer):\n",
        "    tokens = tokenizer(sentence)\n",
        "    ids = torch.tensor(vocab(tokens), dtype=torch.long).unsqueeze(0)\n",
        "    pad_mask = ids == vocab['<pad>']\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        _, attention_raw = model(\n",
        "            ids, src_key_padding_mask=pad_mask, return_attn=True\n",
        "        )\n",
        "        attention = np.asarray(attention_raw[0].detach().cpu().tolist())\n",
        "\n",
        "    return attention\n",
        "\n",
        "attention = get_attention_matrix(model, visualization_sentence, vocab, tokenizer)\n",
        "print(\"Attention matrix:\")\n",
        "print(attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43",
      "metadata": {
        "id": "43"
      },
      "source": [
        "We can interpret the entry in `[row, column] = [i, j]` in the attention matrix as the weight of a query `j` to be used in a linear update of key `i`. For a simple network with a single attention mechanism, this tells you which query words are important to update the meaning of each key word. Thus, if we plot the attention matrix, we can see which words in the sentence influence the meanings of other words (according to the model).\n",
        "\n",
        "We have written a very simple visualization of the sentence, which we can observe below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44",
      "metadata": {
        "id": "44"
      },
      "outputs": [],
      "source": [
        "from utilities import visualization\n",
        "\n",
        "tokens = tokenizer(visualization_sentence)\n",
        "visualization.plot_attention(tokens, attention)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45",
      "metadata": {
        "id": "45"
      },
      "source": [
        "You might find, for example, that the word `please` is influencing the meaning of the word `email`, while it is influence by the words `send` and `this`.\n",
        "\n",
        "Let's look at the numerical importance of the top `(key, query)` pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46",
      "metadata": {
        "id": "46"
      },
      "outputs": [],
      "source": [
        "# Find top (key, query) pairs from attention matrix\n",
        "def find_top_k_pairs(attention, sentence, tokenizer, k=5):\n",
        "    tokens = tokenizer(sentence)\n",
        "    top_k_indices = np.unravel_index(np.argsort(attention.ravel())[-k:], attention.shape)\n",
        "    top_k_pairs = [(tokens[i], tokens[j]) for i, j in zip(*top_k_indices)]\n",
        "    attention_values = [attention[i, j] for i, j in zip(*top_k_indices)]\n",
        "\n",
        "    top_k_pairs = sorted(zip(top_k_pairs, attention_values), key=lambda x: x[1], reverse=True)\n",
        "    return top_k_pairs\n",
        "\n",
        "def print_top_k_pairs(attention, sentence, tokenizer, k=5):\n",
        "    top_k_pairs = find_top_k_pairs(attention, sentence, tokenizer, k=k)\n",
        "    print(\"Top (query, key) pairs\")\n",
        "    print(\"query -> key : attention\")\n",
        "    print(\"------------------------\")\n",
        "    print(\"\")\n",
        "    for (query, key), pair_attn in top_k_pairs:\n",
        "        print(f\"{query} â†’ {key} : {pair_attn:.4f}\")\n",
        "\n",
        "print_top_k_pairs(attention, visualization_sentence, tokenizer, k=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47",
      "metadata": {
        "id": "47"
      },
      "source": [
        "Do these results make sense to you?\n",
        "\n",
        "We can also sum over the queries to see which keys are the most attended to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48",
      "metadata": {
        "id": "48"
      },
      "outputs": [],
      "source": [
        "visualization.display_sentence_with_alpha(visualization_sentence, tokenizer, attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49",
      "metadata": {
        "id": "49"
      },
      "source": [
        "Let's package everything into a single function for easy visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50",
      "metadata": {
        "id": "50"
      },
      "outputs": [],
      "source": [
        "def run_visualization(model, sentence, vocab, tokenizer):\n",
        "    print_prediction_for_sentence(sentence, whitespace=False)\n",
        "\n",
        "    attention = get_attention_matrix(model, sentence, vocab, tokenizer)\n",
        "\n",
        "    visualization.display_sentence_with_alpha(sentence, tokenizer, attention)\n",
        "\n",
        "    tokens = tokenizer(sentence)\n",
        "    visualization.plot_attention(tokens, attention)\n",
        "\n",
        "    print_top_k_pairs(attention, sentence, tokenizer, k=8)\n",
        "\n",
        "run_visualization(model, visualization_sentence, vocab, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51",
      "metadata": {
        "id": "51"
      },
      "source": [
        "Now it's your turn! Play around with a few sentences. What do you notice? Is the model focusing on what you expect?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52",
      "metadata": {
        "id": "52"
      },
      "outputs": [],
      "source": [
        "test_sentence = \"Your sentence here.\"\n",
        "\n",
        "run_visualization(model, test_sentence, vocab, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53",
      "metadata": {
        "id": "53"
      },
      "source": [
        "## Extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54",
      "metadata": {
        "id": "54"
      },
      "source": [
        "If you have gotten to this point and are bored, here are some suggested extensions for you."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55",
      "metadata": {
        "id": "55"
      },
      "source": [
        "### Play around with the model code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56",
      "metadata": {
        "id": "56"
      },
      "source": [
        "Take a look at the implementation of the model. Read the functions and see if they make sense to you. Feel free to edit the model topology and see what happens!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57",
      "metadata": {
        "id": "57"
      },
      "source": [
        "### Add additional layers to the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58",
      "metadata": {
        "id": "58"
      },
      "source": [
        "The model is constructed to allow for the parameter `num_layers`. Each layer is a combination of an attention mechanism and a multilayer perceptron network. What happens if you increase the number of layers from 1 to a higher number? How does the performance of the model change? Does the visualization still make as much sense?\n",
        "\n",
        "Note that the visualization will always draw from the *last* attention layer. After the first attention layer, the token vectors have already been updated, so it is much more difficult to interpret the meaning of subsequent attention matrices."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59",
      "metadata": {
        "id": "59"
      },
      "source": [
        "### Augment the training sample"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60",
      "metadata": {
        "id": "60"
      },
      "source": [
        "Maybe there are some words that are not present in the vocabulary yet, but are relevant to you. These words will not be interpretable by the model, since it has never seen them. The solution is to get more training data!\n",
        "\n",
        "For now, the easiest way to generate the training data is to do it by hand. Try adding some request and non-request examples to the training dataset, and make sure that they use words that don't exist in the dataset yet (physics-related words or words about social media are good candidates here). See how the old model treats sentences that use these words (question: how would these words be treated before they are present in the vocabulary?), then train a new model with an updated vocabulary, and look at how the performance changes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61",
      "metadata": {
        "id": "61"
      },
      "source": [
        "### [Hard] Train a model for a new task"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62",
      "metadata": {
        "id": "62"
      },
      "source": [
        "You can try to train a model for a new task. Many text datasets can be found online. The dataset <https://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/deisear/> for example contains many examples of English and German sentences, classified by expressed emotion. You could download this dataset, perform the necessary preprocessing, and train a model to classify emotions.\n",
        "\n",
        "A few things to note:\n",
        "\n",
        "- The dataset has the emotion removed from each sentence. It might be helpful to preprocess the sentences to restore the emotion, in order to train over complete sentences.\n",
        "- There are more than 2 emotions in the dataset. You could train in one of two ways:\n",
        "  - Classify emotions as positive or negative, and then train a classifier to determine whether a given sentence is positive or negative. This will allow you to use the model that has already been written.\n",
        "  - Classify the emotion itself. This will require you to encode the emotions under a \"one-hot encoding,\" where the sentence labels are vectors of length `n_emotions`. Then each emotion is assigned an entry in the vector, so that a label for a given emotion would have a `1` in that place and a `0` elsewhere. For example, imagine you have 3 emotions: `happy`, `sad`, and `calm`. Then a label for `happy` could be `[1, 0, 0]`, a label for `sad` could be `[0, 1, 0]`, etc. The model will need to be updated to output vectors into an `n_emotions`-dimensional space.\n",
        "- If you use the German-language sentences, you will need a German-language tokenizer. The conda environment specified in `environment.yml` includes `spaCy`, which is a tokenizer with a list of languages available here: <https://spacy.io/usage/models#section-languages>\n",
        "\n",
        "To load a new tokenizer, you can download the dictionary with, e.g.,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63",
      "metadata": {
        "id": "63"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download de_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64",
      "metadata": {
        "id": "64"
      },
      "source": [
        "and then load the tokenizer with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65",
      "metadata": {
        "id": "65"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "tokenizer_de = spacy.load(\"de_core_news_sm\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ErUM",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}